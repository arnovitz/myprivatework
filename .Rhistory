add_model(log_spec)   # add your model spec
# 2. Decision tree, using the `C5.0` engine
# 3. Random Forest, using  the `ranger` engine and setting `importance = "impurity"`)
# 4. A boosted tree using Extreme Gradient Boosting, and the `xgboost` engine
# 5. A k-nearest neighbours,  using 4 nearest_neighbors and the `kknn` engine
# Logistic regression
log_spec <-  logistic_reg() %>%  # model type
set_engine(engine = "glm") %>%  # model engine
set_mode("classification") # model mode
# Show your model specification
log_spec
# Decision Tree
tree_spec <- decision_tree() %>%
set_engine(engine = "C5.0") %>%
set_mode("classification")
tree_spec
# Random Forest
library(ranger)
rf_spec <-
rand_forest() %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
# Boosted tree (XGBoost)
library(xgboost)
xgb_spec <-
boost_tree() %>%
set_engine("xgboost") %>%
set_mode("classification")
# K-nearest neighbour (k-NN)
knn_spec <-
nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors
set_engine("kknn") %>%
set_mode("classification")
log_wflow <- # new workflow object
workflow() %>% # use workflow function
add_recipe(fraud_rec) %>%   # use the new recipe
add_model(log_spec)   # add your model spec
# show object
log_wflow
tree_wflow <-
workflow() %>%
add_recipe(fraud_rec) %>%
add_model(tree_spec)
rf_wflow <-
workflow() %>%
add_recipe(fraud_rec) %>%
add_model(rf_spec)
xgb_wflow <-
workflow() %>%
add_recipe(fraud_rec) %>%
add_model(xgb_spec)
knn_wflow <-
workflow() %>%
add_recipe(fraud_rec) %>%
add_model(knn_spec)
log_wflow <- # new workflow object
workflow() %>% # use workflow function
add_recipe(fraud_rec) %>%   # use the new recipe
add_model(log_spec)   # add your model spec
tic()
tic()
log_res <- log_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
recall, precision, f_meas, accuracy,
kap, roc_auc, sens, spec),
control = control_resamples(save_pred = TRUE))
time <- toc()
log_time <- time[[4]]
## Model Comparison
log_metrics <-
log_res %>%
collect_metrics(summarise = TRUE) %>%
# add the name of the model to every row
mutate(model = "Logistic Regression",
time = log_time)
# add mode models here
# create dataframe with all models
model_compare <- bind_rows(log_metrics,
tree_metrics,
rf_metrics,
xgb_metrics,
knn_metrics
) %>%
# get rid of 'sec elapsed' and turn it into a number
mutate(time = str_sub(time, end = -13) %>%
as.double()
)
# create dataframe with all models
model_compare <- bind_rows(log_metrics,
tree_metrics,
rf_metrics,
xgb_metrics,
knn_metrics
) %>%
# get rid of 'sec elapsed' and turn it into a number
mutate(time = str_sub(time, end = -13) %>%
as.double()
)
log_metrics <-
log_res %>%
collect_metrics(summarise = TRUE) %>%
# add the name of the model to every row
mutate(model = "Logistic Regression")
tree_metrics <-
tree_res %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "Decision Tree")
M
M
log_res <- log_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
recall, precision, f_meas, accuracy,
kap, roc_auc, sens, spec),
control = control_resamples(save_pred = TRUE))
# Show average performance over all folds (note that we use log_res):
log_res %>%  collect_metrics(summarize = TRUE)
# Show performance for every single fold:
log_res %>%  collect_metrics(summarize = FALSE)
log_pred <- log_res %>% collect_predictions()
log_pred %>%  conf_mat(test, .pred_class)
log_pred %>%
conf_mat(test, .pred_class) %>%
autoplot(type = "mosaic") +
geom_label(aes(
x = (xmax + xmin) / 2,
y = (ymax + ymin) / 2,
label = c("TP", "FN", "FP", "TN")))
log_pred %>%  conf_mat(test, .pred_class)
log_pred %>%  conf_mat(card_fraud_test, .pred_is_fraud)
log_pred %>%  conf_mat(card_fraud_test, .pred_1)
log_pred %>%  conf_mat(card_fraud_test, .pred_class)
View(log_pred)
log_pred %>%  conf_mat(is_fraud, .pred_class)
log_pred %>%
conf_mat(test, .pred_class) %>%
autoplot(type = "mosaic") +
geom_label(aes(
x = (xmax + xmin) / 2,
y = (ymax + ymin) / 2,
label = c("TP", "FN", "FP", "TN")))
log_pred %>%
conf_mat(is_fraud, .pred_class) %>%
autoplot(type = "mosaic") +
geom_label(aes(
x = (xmax + xmin) / 2,
y = (ymax + ymin) / 2,
label = c("TP", "FN", "FP", "TN")))
log_pred %>%
conf_mat(test, .pred_class) %>%
autoplot(type = "heatmap")
log_pred %>%
conf_mat(is_fraud, .pred_class) %>%
autoplot(type = "heatmap")
log_pred %>%
group_by(id) %>% # id contains our folds
roc_curve(test, .pred_Fail) %>%
autoplot()
log_pred %>%
group_by(id) %>% # id contains our folds
roc_curve(is_fraud, .pred_1) %>%
autoplot()
tree_res <-
tree_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
recall, precision, f_meas,
accuracy, kap,
roc_auc, sens, spec),
control = control_resamples(save_pred = TRUE)
)
tree_res <-
tree_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
recall, precision, f_meas,
accuracy, kap,
roc_auc, sens, spec),
control = control_resamples(save_pred = TRUE)
)
tree_res %>%  collect_metrics(summarize = TRUE)
rf_res <-
rf_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
recall, precision, f_meas,
accuracy, kap,
roc_auc, sens, spec),
control = control_resamples(save_pred = TRUE)
)
rf_res <-
rf_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
recall, precision, f_meas,
accuracy, kap,
roc_auc, sens, spec),
control = control_resamples(save_pred = TRUE)
)
rf_res %>%  collect_metrics(summarize = TRUE)
log_pred %>%
group_by(id) %>% # id contains our folds
roc_curve(is_fraud, .pred_1) %>%
autoplot()
tree_res <-
tree_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
recall, precision, f_meas,
accuracy, kap,
roc_auc, sens, spec),
control = control_resamples(save_pred = TRUE)
)
tree_res %>%  collect_metrics(summarize = TRUE)
rf_res <-
rf_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
recall, precision, f_meas,
accuracy, kap,
roc_auc, sens, spec),
control = control_resamples(save_pred = TRUE)
)
rf_res <-
rf_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
recall, precision, f_meas,
accuracy, kap,
roc_auc, sens, spec),
control = control_resamples(save_pred = TRUE)
)
rf_res %>%  collect_metrics(summarize = TRUE)
xgb_res <-
xgb_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
recall, precision, f_meas,
accuracy, kap,
roc_auc, sens, spec),
control = control_resamples(save_pred = TRUE)
)
xgb_res %>% collect_metrics(summarize = TRUE)
knn_res <-
knn_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
recall, precision, f_meas,
accuracy, kap,
roc_auc, sens, spec),
control = control_resamples(save_pred = TRUE)
)
knn_res <-
knn_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
recall, precision, f_meas,
accuracy, kap,
roc_auc, sens, spec),
control = control_resamples(save_pred = TRUE)
)
knn_res %>% collect_metrics(summarize = TRUE)
knn_res <-
knn_wflow %>%
fit_resamples(
resamples = cv_folds,
metrics = metric_set(
recall, precision, f_meas,
accuracy, kap,
roc_auc, sens, spec),
control = control_resamples(save_pred = TRUE)
)
knn_res %>% collect_metrics(summarize = TRUE)
log_metrics <-
log_res %>%
collect_metrics(summarise = TRUE) %>%
# add the name of the model to every row
mutate(model = "Logistic Regression")
tree_metrics <-
tree_res %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "Decision Tree")
rf_metrics <-
rf_res %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "Random Forest")
xgb_metrics <-
xgb_res %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "XGBoost")
knn_metrics <-
knn_res %>%
collect_metrics(summarise = TRUE) %>%
mutate(model = "Knn")
# create dataframe with all models
model_compare <- bind_rows(log_metrics,
tree_metrics,
rf_metrics,
xgb_metrics,
knn_metrics)
log_metrics <-
log_res %>%
collect_metrics(summarise = TRUE) %>%
# add the name of the model to every row
mutate(model = "Logistic Regression",
time = log_time)
# create dataframe with all models
model_compare <- bind_rows(log_metrics,
tree_metrics,
rf_metrics,
xgb_metrics,
knn_metrics
) %>%
# get rid of 'sec elapsed' and turn it into a number
mutate(time = str_sub(time, end = -13) %>%
as.double()
)
#Pivot wider to create barplot
model_comp <- model_compare %>%
select(model, .metric, mean, std_err) %>%
pivot_wider(names_from = .metric, values_from = c(mean, std_err))
# show mean are under the curve (ROC-AUC) for every model
model_comp %>%
arrange(mean_roc_auc) %>%
mutate(model = fct_reorder(model, mean_roc_auc)) %>% # order results
ggplot(aes(model, mean_roc_auc, fill=model)) +
geom_col() +
coord_flip() +
scale_fill_brewer(palette = "Blues") +
geom_text(
size = 3,
aes(label = round(mean_roc_auc, 2),
y = mean_roc_auc + 0.08),
vjust = 1
)+
theme_light()+
theme(legend.position = "none")+
labs(y = NULL)
# show mean are under the curve (ROC-AUC) for every model
model_comp %>%
arrange(mean_roc_auc) %>%
mutate(model = fct_reorder(model, mean_roc_auc)) %>% # order results
ggplot(aes(model, mean_roc_auc, fill=model)) +
geom_col() +
coord_flip() +
scale_fill_brewer(palette = "Blues") +
geom_text(
size = 3,
aes(label = round(mean_roc_auc, 2),
y = mean_roc_auc + 0.08),
vjust = 1
)+
theme_light()+
theme(legend.position = "none")+
labs(y = NULL)
last_fit_xgb <- last_fit(xgb_wflow,
split = data_split,
metrics = metric_set(
accuracy, f_meas, kap, precision,
recall, roc_auc, sens, spec))
last_fit_xgb %>% collect_metrics(summarize = TRUE)
#Compare to training
xgb_res %>% collect_metrics(summarize = TRUE)
library(vip)
last_fit_xgb %>%
pluck(".workflow", 1) %>%
pull_workflow_fit() %>%
vip(num_features = 10) +
theme_light()
last_fit_xgb %>%
collect_predictions() %>%
conf_mat(test, .pred_class) %>%
autoplot(type = "heatmap")
last_fit_xgb %>%
collect_predictions() %>%
conf_mat(is_fraud, .pred_class) %>%
autoplot(type = "heatmap")
## Final ROC curve
last_fit_xgb %>%
collect_predictions() %>%
roc_curve(test, .pred_Fail) %>%
autoplot()
## Final ROC curve
last_fit_xgb %>%
collect_predictions() %>%
roc_curve(is_fraud, .pred_Fail) %>%
autoplot()
## Final ROC curve
last_fit_xgb %>%
collect_predictions() %>%
roc_curve(is_fraud, .pred_1) %>%
autoplot()
log_metrics <-
log_res %>%
collect_metrics(summarise = TRUE) %>%
# add the name of the model to every row
mutate(model = "Logistic Regression",
time = log_time)
# create dataframe with all models
model_compare <- bind_rows(log_metrics,
tree_metrics,
rf_metrics,
xgb_metrics,
knn_metrics
) %>%
# get rid of 'sec elapsed' and turn it into a number
mutate(time = str_sub(time, end = -13) %>%
as.double()
)
last_fit_xgb %>%
collect_predictions() %>%
conf_mat(is_fraud, .pred_class) %>%
autoplot(type = "heatmap")
## Final ROC curve
last_fit_xgb %>%
collect_predictions() %>%
roc_curve(is_fraud, .pred_1) %>%
autoplot()
last_fit_xgb %>%
collect_predictions() %>%
conf_mat(is_fraud, .pred_class) %>%
autoplot(type = "heatmap")
## Final ROC curve
last_fit_xgb %>%
collect_predictions() %>%
roc_curve(is_fraud, .pred_1) %>%
autoplot()
best_model_preds <-
best_model_wflow %>%
fit(data = card_fraud_train) %>%
## Use `augment()` to get predictions for entire data set
augment(new_data = card_fraud)
best_model_preds <-
rf_res %>%
fit(data = card_fraud_train) %>%
## Use `augment()` to get predictions for entire data set
augment(new_data = card_fraud)
best_model_preds <-
rf_wflow %>%
fit(data = card_fraud_train) %>%
## Use `augment()` to get predictions for entire data set
augment(new_data = card_fraud)
best_model_preds <-
rf_wflow %>%
fit(data = card_fraud_train) %>%
## Use `augment()` to get predictions for entire data set
augment(new_data = card_fraud)
best_model_preds %>%
conf_mat(truth = is_fraud, estimate = .pred_class)
cost <- best_model_preds %>%
select(is_fraud, amt, pred = .pred_class)
cost <- cost %>%
mutate(
# naive false-- we think every single transaction is ok and not fraud
# false negatives-- we thought they were not fraud, but they were
# false positives-- we thought they were fraud, but they were not
# true positives-- we thought they were fraud, and they were
# true negatives-- we thought they were ok, and they were
)
cost_summary <- cost %>%
summarise(across(starts_with(c("false","true", "amt")),
~ sum(.x, na.rm = TRUE)))
cost_summary
cost <- cost %>%
mutate(naive_false <- sum(prediction != truth),
false_negatives <- sum(prediction == 0 & truth == 1),
false_positives <- sum(prediction == 1 & truth == 0),
true_positives <- sum(prediction == 1 & truth == 1),
true_negatives <- sum(prediction ==0&truth==0))
cost <- cost %>%
mutate(naive_false <- sum(prediction != truth),
false_negatives <- sum(prediction == 0 & truth == 1),
false_positives <- sum(prediction == 1 & truth == 0),
true_positives <- sum(prediction == 1 & truth == 1),
true_negatives <- sum(prediction ==0&truth==0))
cost <- cost %>%
mutate(naive_false <- sum(prediction != is_fraud),
false_negatives <- sum(prediction == 0 & is_fraud == 1),
false_positives <- sum(prediction == 1 & is_fraud == 0),
true_positives <- sum(prediction == 1 & is_fraud == 1),
true_negatives <- sum(prediction ==0&is_fraud==0))
View(cost)
cost <- cost %>%
mutate(naive_false <- sum(pred != is_fraud),
false_negatives <- sum(pred == 0 & is_fraud == 1),
false_positives <- sum(pred == 1 & is_fraud == 0),
true_positives <- sum(pred == 1 & is_fraud == 1),
true_negatives <- sum(pred ==0&is_fraud==0))
cost_summary <- cost %>%
summarise(across(starts_with(c("false","true", "amt")),
~ sum(.x, na.rm = TRUE)))
cost_summary
cost_summary <- cost %>%
summarise(across(starts_with(c("false","true", "amt")),
~ sum(.x, na.rm = TRUE)))
cost_summary
scales::dollar(cost_summary$false_negatives)
View(cost_summary)
glimpse(cost_summary)
